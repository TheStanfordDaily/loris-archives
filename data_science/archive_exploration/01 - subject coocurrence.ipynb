{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive data\n",
    "The Wellcome archive sits in a collections management system called CALM, which follows a rough set of standards and guidelines for storing archival records called [ISAD(G)](https://en.wikipedia.org/wiki/ISAD(G). The archive is comprised of _collections_, each of which has a hierarcal set of series, sections, subjects, items and pieces sitting underneath it.  \n",
    "In the following notebooks I'm going to explore it and try to make as much sense of it as I can programatically.\n",
    "\n",
    "Let's start by loading in a few useful packages and defining some nice utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from umap import UMAP\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list):\n",
    "    return [item for sublist in list for item in sublist]\n",
    "\n",
    "def cartesian(*arrays):\n",
    "    return np.array([x.reshape(-1) for x in np.meshgrid(*arrays)]).T\n",
    "\n",
    "def clean_subject(subject):\n",
    "    return subject.strip().lower().replace('<p>', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's load up our CALM data (exported in its entirity as a single `.json`, where each line is a record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/calm_records.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype(str).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring individual columns\n",
    "At the moment I have no idea what kind of information CALM contains - lets look at the list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm looking through a sample of values in each column, choosing the columns to explore based on the their headings, a bit of contextual info from colleagues and the `df.describe()` above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After much trial and error...\n",
    "Subjects look like an interesting avenue to explore further. Where subjects have _actually_ been filled in and the entry is not `None`, a list of subjects is returned.  \n",
    "We can explore some of these subjects' subtleties by creating an adjacency matrix. We'll count the number of times each subject appears alongside every other subject and return a big $n \\times n$ matrix, where $n$ is the total number of unique subjects.  \n",
    "We can use this adjacency matrix for all sorts of stuff, but we have to build it first. To start, lets get a uniqur list of all subjects. This involves unpacking each sub-list and flattening them out into one long list, before finding the unique elements. We'll then use the `clean_subject` function defined above to get rid of any irregularities which might become annoying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(set(flatten(df['Subject'].dropna().tolist())))\n",
    "clean_subjects = list(map(clean_subject, subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it's often helpful to _index_ our data, ie transform words into numbers. We'll create two dictionaries which map back and forth between the subjects and their corresponding indicies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_subject = {index: subject for index, subject in enumerate(clean_subjects)}\n",
    "subject_to_index = {subject: index for index, subject in enumerate(subjects)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets instantiate an empty numpy array which we'll then fill with our coocurrence data. Each column and each row will represent a subject - each cell (the intersection of a column and row) will therefore represent the 'strength' of the interaction between those subjects. As we haven't seen any interactions yet, we'll set every array element to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = np.empty((len(subjects), len(subjects)), \n",
    "                     dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To populate the matrix, we want to find every possible combination of subject in each sub-list from our original column, ie if we had the subjects\n",
    "\n",
    "`[Disease, Heart, Heart Diseases, Cardiology]`\n",
    "\n",
    "we would want to return \n",
    "\n",
    "`\n",
    "[['Disease', 'Disease'],\n",
    " ['Heart', 'Disease'],\n",
    " ['Heart Diseases', 'Disease'],\n",
    " ['Cardiology', 'Disease'],\n",
    " ['Disease', 'Heart'],\n",
    " ['Heart', 'Heart'],\n",
    " ['Heart Diseases', 'Heart'],\n",
    " ['Cardiology', 'Heart'],\n",
    " ['Disease', 'Heart Diseases'],\n",
    " ['Heart', 'Heart Diseases'],\n",
    " ['Heart Diseases', 'Heart Diseases'],\n",
    " ['Cardiology', 'Heart Diseases'],\n",
    " ['Disease', 'Cardiology'],\n",
    " ['Heart', 'Cardiology'],\n",
    " ['Heart Diseases', 'Cardiology'],\n",
    " ['Cardiology', 'Cardiology']]\n",
    "`\n",
    "\n",
    "The `cartesian()` function which I've defined above will do that for us. We then find the appropriate intersection in the matrix and add another unit of 'strength' to it.  \n",
    "We'll do this for every row of subjects in the `['Subjects']` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_of_subjects in tqdm(df['Subject'].dropna()):\n",
    "    for subject_pair in cartesian(row_of_subjects, row_of_subjects):\n",
    "        subject_index_1 = subject_to_index[subject_pair[0]]\n",
    "        subject_index_2 = subject_to_index[subject_pair[1]]\n",
    "\n",
    "        adjacency[subject_index_1, subject_index_2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do all sorts of fun stiff now - adjacency matrices are the basis on which all of graph theory happens. Because it's a bit more interesting, I'm going to start with some dimensionality reduction.\n",
    "Using [UMAP](https://github.com/lmcinnes/umap), we can squash the $n \\times n$ dimensional matrix down into a $n \\times m$ dimensional one, where $m$ is some arbitrary integer. Setting $m$ to 2 will allow us to plot each subject as a point on a two dimensional plane. UMAP will try to preserve the 'distances' between subjects - in this case, that means that similar subjects will end up clustered together, and different subjects will move apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_2d = pd.DataFrame(UMAP(n_components=2)\n",
    "                            .fit_transform(adjacency))\n",
    "\n",
    "embedding_2d.plot.scatter(x=0, y=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can isolate the clusters we've found above using a number of different methods - `scikit-learn` provides easy access to some very powerful algorithms. Here I'll use a technique called _agglomerative clustering_, and make a guess that 15 is an appropriate number of clusters to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 15\n",
    "embedding_2d['labels'] = AgglomerativeClustering(n_clusters).fit_predict(embedding_2d.values)\n",
    "embedding_2d.plot.scatter(x=0, y=1, c='labels', cmap='Paired');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `index_to_subject` mapping that we created earlier to examine which subjects have been grouped together into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(str(i) + ' ' + '-'*80 + '\\n')\n",
    "    print(np.sort([clean_subject(index_to_subject[index])\n",
    "                    for index in embedding_2d[embedding_2d['labels'] == i].index.values]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
