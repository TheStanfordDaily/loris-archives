{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masked language model after BERT\n",
    "heavily influenced by the [BERT paper](https://arxiv.org/abs/1810.04805) and [codertimo's pytorch implementation](https://github.com/codertimo/BERT-pytorch), which itself borrows heavily on the code from [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import string\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define multi-headed attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    \n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [(layer(x)\n",
    "                              .view(batch_size, -1, self.h, self.d_k)\n",
    "                              .transpose(1, 2))\n",
    "                             for layer, x in zip(self.linear_layers, \n",
    "                                                 (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a couple of utils defining `SublayerConnection()` and `PositionwiseFeedForward()`, which contribute to the transformer block. Most of this stuff comes from [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) page, with a little bit of modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module\"\n",
    "    def __init__(self, wv_size, epsilon=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(wv_size))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(wv_size))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.epsilon) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"A resnet connection, followed by a layer norm.\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"GELU paper here https://arxiv.org/abs/1606.08415\"\n",
    "    def forward(self, x):\n",
    "        a = math.sqrt(2 / math.pi)\n",
    "        b = x + 0.044715 * torch.pow(x, 3)\n",
    "        c = 1 + torch.tanh(a * b)\n",
    "        return 0.5 * x * c\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple feed forward network with GELU.\n",
    "    d_ff (the size of the hidden layer) is here taken to be 2*d_model\n",
    "    (the input size), so the network has a good amount of room to shift \n",
    "    itself around in these layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                          GELU(),\n",
    "                                          nn.Dropout(dropout),\n",
    "                                          nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=hidden*2, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the full BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_embedding, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param pretrained_embedding: our pre-trained word embeddings\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # modifying what's in the paper here - instead of doing the whole weirdo \n",
    "        # BERTEmbedding thing, we just use the established fasttext embeddings. These \n",
    "        # can happily be fine-tuned, but we avoid all of the unknowable unseeable \n",
    "        # complications that come with the method in the paper. Also, why are they \n",
    "        # using weird word embedding sizes?? 300d has been shown to be plenty... Very odd\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "        self.emb_to_hidden = nn.Linear(pretrained_embedding.shape[1], hidden)\n",
    "        \n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, dropout) \n",
    "             for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        # embedding the indexed sequence to sequence of vectors. then bump up\n",
    "        # pretrained embeddings into the correct d space for our network\n",
    "        x = self.embedding(x)\n",
    "        x = self.emb_to_hidden(x)\n",
    "        \n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        # after going through the transformer we switch back into the original wv space\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is then used to build a masked-language-model and next-sentence-predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super(MaskedLanguageModel, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super(NextSentencePrediction, self).__init__()\n",
    "        self.linears = nn.Sequential(nn.Linear(hidden_size*2, hidden_size//2),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(0.2),\n",
    "                                     nn.Linear(hidden_size//2, 2))\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        concatenated = torch.cat([x1[:, 0], \n",
    "                                  x2[:, 0]], 1)\n",
    "        classification = self.linears(concatenated)\n",
    "        return self.softmax(classification)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_path = '/Users/pimh/datasets/crawl-300d-2M.vec'\n",
    "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "fasttext = {line.split()[0]: np.array(line.split()[1:])\n",
    "            for line in tqdm(list(wv_file)[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(fasttext.keys())}\n",
    "\n",
    "word_to_index['<UNK>'] = len(fasttext)\n",
    "word_to_index['<MASK>'] = len(fasttext)\n",
    "word_to_index['<PAD>'] = len(fasttext)\n",
    "\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedding_matrix = torch.Tensor(np.array(list(fasttext.values())).astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_wv = fasttext_embedding_matrix.mean(dim=0).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedding_matrix = torch.cat([fasttext_embedding_matrix,\n",
    "                                       mean_wv, mean_wv, mean_wv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# books"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts = []\n",
    "text_ids = np.random.choice(np.arange(1000, 1100), \n",
    "                            size=10, replace=False)\n",
    "\n",
    "for n in tqdm(text_ids):\n",
    "    url = 'http://www.gutenberg.org/files/{}/{}.txt'.format(n, n)\n",
    "    response = requests.get(url).text\n",
    "    text = ' '.join(response.split())\n",
    "    texts.append(nlp(text))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lengths = []\n",
    "for text in texts:\n",
    "    lengths += [len(sentence) for sentence in list(text.sents)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text = np.random.choice(texts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series([str(word) for word in text]).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences = list(text.sents)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series([3 < len(s) < 60 for s in sentences]).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence = np.array(np.random.choice(sentences))\n",
    "print(' '.join([str(w) for w in sentence]))\n",
    "\n",
    "replace_indexes = np.random.choice(len(sentence))\n",
    "sentence[replace_indexes] = '<MASK>'\n",
    "\n",
    "print(' '.join([str(w) for w in sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# three types of dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indexes(sentence):\n",
    "    '''turn a spacy sentence into a list of usable indexes'''\n",
    "    tokens = [str(token) for token in sentence]\n",
    "    indexes = [word_to_index[token] \n",
    "               if token in word_to_index\n",
    "               else word_to_index['<UNK>'] \n",
    "               for token in tokens]\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def pad_sequence(sentences, pad_length=None):\n",
    "    if pad_length is None:\n",
    "        pad_length = max([len(sent) for sent in sentences])\n",
    "\n",
    "    padded = np.full((len(sentences), pad_length), word_to_index['<PAD>'])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        padded[i][pad_length - len(sentence):] = sentence\n",
    "    return padded\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    masked, target = zip(*batch)\n",
    "    \n",
    "    batch_size = len(target)\n",
    "    seq_length = max([len(s) for s in target])\n",
    "\n",
    "    padded_masked = pad_sequence(masked, pad_length=seq_length)\n",
    "    padded_target = pad_sequence(target, pad_length=seq_length)\n",
    "    \n",
    "    return torch.LongTensor(padded_masked), torch.LongTensor(padded_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(index_sequence):\n",
    "    '''\n",
    "    takes an input sequence of indexes and randomly chooses one of \n",
    "    them to be modified. The chosen index is:\n",
    "        - with p=0.8 : replaced with a specific <MASK> token\n",
    "        - with p=0.1 : a random token from the vocabulary\n",
    "        - with p=0.1 : left unchanged\n",
    "    '''\n",
    "    masked_sequence = index_sequence.copy()\n",
    "    replace_index = random.randint(0, len(index_sequence)-1)\n",
    "    \n",
    "    p = random.random()\n",
    "    if p < 0.8: masked_sequence[replace_index] = word_to_index['<MASK>']\n",
    "    elif p > 0.9: masked_sequence[replace_index] = random.randint(0, len(word_to_index)-1)\n",
    "\n",
    "    return masked_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, index_lists):\n",
    "        self.index_lists = index_lists\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target = self.index_lists[index]\n",
    "        masked = random_mask(target)\n",
    "        return masked, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_sentences = set([s for s in sentences if 5 < len(s) < 50])\n",
    "index_list = [sentence_to_indexes(s) for s in list(acceptable_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * len(indexes)\n",
    "train_indexes = index_list[:train_size]\n",
    "test_indexes = index_list[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlm_dataset = MLMDataset(train_indexes)\n",
    "test_mlm_dataset = MLMDataset(test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "train_mlm = DataLoader(dataset=train_mlm_dataset,\n",
    "                       batch_size=batch_size,\n",
    "                       num_workers=5,\n",
    "                       shuffle=True,\n",
    "                       collate_fn=custom_collate_fn)\n",
    "\n",
    "test_mlm = DataLoader(dataset=test_mlm_dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      num_workers=5,\n",
    "                      collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(acceptable_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp = {}\n",
    "i = 0\n",
    "\n",
    "for j in tqdm(range(len(sentences) - 1)):\n",
    "    s1, s2 = sentences[j:j+2]\n",
    "    if s1 in acceptable_sentences and s2 in acceptable_sentences:\n",
    "        nsp[i] = {'s1': sentence_to_indexes(s1),\n",
    "                  's2': sentence_to_indexes(s2),\n",
    "                  'label': 1}\n",
    "        i += 1\n",
    "    else:\n",
    "        random_sentence = random.choice(list(acceptable_sentences))\n",
    "        indexed_random_sentence = sentence_to_indexes(random_sentence)\n",
    "\n",
    "        if s1 in acceptable_sentences:\n",
    "            nsp[i] = {'s1': sentence_to_indexes(s1), \n",
    "                      's2': indexed_random_sentence,\n",
    "                      'label': 0}\n",
    "            i += 1\n",
    "        if s2 in acceptable_sentences:\n",
    "            nsp[i] = {'s1': indexed_random_sentence,\n",
    "                      's2': sentence_to_indexes(s2),\n",
    "                      'label': 0}\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short to long equivalence\n",
    "iterate over sentences and find adjectives, nouns, and adjective-noun-pairs, and create equivalence between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stle = {}\n",
    "\n",
    "for sentence in tqdm(list(acceptable_sentences)):\n",
    "    for word in sentence:\n",
    "        if word.pos_ in ['NOUN', 'ADJ']:\n",
    "            stle[i] = {'short': sentence_to_indexes([str(word)]),\n",
    "                       'long': sentence_to_indexes(sentence)}\n",
    "            i += 1\n",
    "\n",
    "    for j in range(len(sentence) - 2):\n",
    "        if len(words[j:j+2]) == 2:\n",
    "            word_1, word_2 = words[j:j+2]\n",
    "            if ((word_1.pos_ == 'ADJ') & (word_2.pos_ == 'NOUN')):\n",
    "                print('fuck')\n",
    "                stle[i] = {'short': sentence_to_indexes([str(word_1), str(word_2)]), \n",
    "                           'long': sentence_to_indexes(sentence)}\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first, just try training one task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = (pd.Series(np.concatenate(index_list))\n",
    "                 .value_counts(normalize=True)\n",
    "                 .sort_index()\n",
    "                 .values)\n",
    "\n",
    "class_weights = torch.Tensor(class_weights)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        loop = tqdm(train_mlm)\n",
    "        for masked, target in loop:\n",
    "            masked = torch.LongTensor(masked)#.cuda(non_blocking=True)\n",
    "            target = torch.LongTensor(target)#.cuda(non_blocking=True)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(masked)\n",
    "            print(preds.shape, target.shape)\n",
    "            loss = loss_function(preds, target)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(BERT(fasttext_embedding_matrix), \n",
    "                      MaskedLanguageModel(hidden_size, vocab_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
    "loss_function = nn.NLLLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model,\n",
    "      train_loader=train_mlm,\n",
    "      loss_function=loss_function,\n",
    "      optimiser=optimiser,\n",
    "      n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one universal training loop for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_function, optimiser, n_epochs, batches_per_epoch):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        task_choice = np.random.choice(['MLM', 'NSP', 'STLE'])\n",
    "        \n",
    "        if task_choice == 'MLM':\n",
    "            for masked, target in train_mlm:\n",
    "                masked = torch.LongTensor(masked).cuda(non_blocking=True)\n",
    "                target = torch.LongTensor(target).cuda(non_blocking=True)\n",
    "                optimiser.zero_grad()\n",
    "                preds = MLM(masked)\n",
    "                loss = mlm_loss_function(preds, target)\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "        \n",
    "        if task_choice == 'NSP':\n",
    "            # do the NSP thing\n",
    "            \n",
    "        if task_choice == 'STLE':\n",
    "            for short, long in train_mlm:\n",
    "                short = torch.LongTensor(short).cuda(non_blocking=True)\n",
    "                long = torch.LongTensor(long).cuda(non_blocking=True)\n",
    "                optimiser.zero_grad()\n",
    "                short_embedding = model(short)\n",
    "                long_embedding = model(long)\n",
    "                loss = mlm_loss_function(preds, long)\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_indexes(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
