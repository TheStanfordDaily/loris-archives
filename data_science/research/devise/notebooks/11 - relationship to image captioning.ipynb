{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_string):\n",
    "    return input_string.translate(str.maketrans('', '', punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_image_path = '/home/jupyter/datasets/coco/images/val2014/'\n",
    "image_paths = [coco_image_path + image_id for image_id in os.listdir(coco_image_path)]\n",
    "image_ids = [path.split('/')[-1].split('.')[0] for path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/coco/captions_val2014.json') as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.merge(pd.DataFrame(meta['images']).set_index('id'),\n",
    "               pd.DataFrame(meta['annotations']).set_index('image_id'), \n",
    "               left_index=True, right_index=True)\n",
    "      .reset_index()\n",
    "      [['caption', 'file_name']]\n",
    "     )\n",
    "\n",
    "df['file_name'] = coco_image_path + df['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caption'] = (df['caption']\n",
    "                 .apply(lambda x: ''.join([c for c in x if c.isalpha() or c.isspace()]))\n",
    "                 .apply(str.lower)\n",
    "                 .apply(lambda x: ' '.join(x.split()))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(df['caption'].values).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.index = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            self.word_to_index[word] = self.index\n",
    "            self.index_to_word[self.index] = word\n",
    "            self.index += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            return self.word_to_index['<unk>']\n",
    "        return self.word_to_index[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary()\n",
    "vocabulary.add_word('<pad>')\n",
    "vocabulary.add_word('<start>')\n",
    "vocabulary.add_word('<unk>')\n",
    "vocabulary.add_word('<end>')\n",
    "\n",
    "for word, count in Counter(all_text).items():\n",
    "    vocabulary.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(df)) < 0.8\n",
    "train_df, test_df = df[mask], df[~mask]\n",
    "\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionsDataset(Dataset):\n",
    "    def __init__(self, path_df, vocab, transform=transforms.ToTensor()):\n",
    "        self.ids = path_df.index.values\n",
    "        self.image_paths = path_df['file_name'].values\n",
    "        self.titles = path_df['caption'].values\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = '<start> ' + self.titles[index] + ' <end>'\n",
    "        tokens = [self.vocab(token) for token in caption.split()]\n",
    "        target = torch.Tensor(tokens)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomGrayscale(0.8),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionsDataset(train_df, vocabulary, transform=transform)\n",
    "test_dataset = CaptionsDataset(test_df, vocabulary, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader\n",
    "with custom `collate_fn` (allowing for variable-length padding on captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)  # descending order\n",
    "    images, captions = zip(*data)\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    targets = torch.zeros(len(captions), max_len).long()\n",
    "    for i, caption in enumerate(captions):\n",
    "        targets[i, :len(caption)] = caption\n",
    "\n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=5,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = {}\n",
    "\n",
    "with open('datasets/wiki.en.vec', encoding='utf-8') as f:\n",
    "    for line in tqdm(f.readlines()[1:100000]):\n",
    "        line = line.split()\n",
    "        word, vector = ' '.join(line[:-300]), np.array(line[-300:]).astype(np.float32)\n",
    "        fasttext[word.lower()] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_max = np.abs(list(fasttext.values())).max()\n",
    "\n",
    "for word in tqdm(fasttext.keys()):\n",
    "    fasttext[word] = fasttext[word] / wv_max\n",
    "\n",
    "wv_mean = np.array(list(fasttext.values())).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext['<pad>'] = np.zeros((300,))\n",
    "fasttext['<start>'] = np.zeros((300,))\n",
    "fasttext['<unk>'] = np.zeros((300,))\n",
    "fasttext['<end>'] = np.zeros((300,))\n",
    "\n",
    "fasttext['<pad>'][0] = 1\n",
    "fasttext['<start>'][1] = 1\n",
    "fasttext['<unk>'][2] = 1\n",
    "fasttext['<end>'][3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fasttext_vectors = [fasttext[word] \n",
    "                        if word in fasttext else wv_mean\n",
    "                        for word in vocabulary.index_to_word.values()]\n",
    "\n",
    "all_fasttext_vectors = torch.Tensor(np.stack(all_fasttext_vectors))\n",
    "all_fasttext_vectors.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fasttext_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        '''Load the pretrained ResNet-152 and replace top fc layer.'''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        for p in resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.bn = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        '''Extract feature vectors from input images.'''\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, n_layers, embedding_matrix, max_seq_length=20):\n",
    "        '''Set the hyper-parameters and build the layers.'''\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        '''Decode image feature vectors and generates captions.'''\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        '''Generate captions for given image features using greedy search.'''\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_size = 300\n",
    "hidden_size = 500\n",
    "n_layers = 1\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(embedding_size).to(device)\n",
    "decoder = DecoderRNN(embedding_size, hidden_size, len(vocabulary), n_layers, all_fasttext_vectors).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, train_loader, epoch, \n",
    "                loss_function, optimiser, device=device):\n",
    "    \n",
    "    loop = tqdm(train_loader)\n",
    "    \n",
    "    for images, captions, lengths in loop:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        captions = captions.cuda(non_blocking=True)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0].to(device)\n",
    "        \n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (list(filter(lambda p: p.requires_grad, decoder.parameters())) +\n",
    "          list(filter(lambda p: p.requires_grad, encoder.parameters())))\n",
    "\n",
    "\n",
    "n_epochs = 4\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_epoch(encoder=encoder, \n",
    "                decoder=decoder,\n",
    "                train_loader=train_loader,\n",
    "                loss_function=nn.CrossEntropyLoss(),\n",
    "                optimiser=Adam(params, lr=learning_rate),\n",
    "                epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
    "ax = loss_data.plot(subplots=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample result from coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "caption, path = test_df.sample().values[0]\n",
    "print(caption)\n",
    "\n",
    "img = load_image(path, transform=transform).to(device)\n",
    "\n",
    "Image.fromarray((img.to('cpu').data.numpy() * 255)\n",
    "                .astype(np.uint8)\n",
    "                .reshape(3, 224, 224)\n",
    "                .transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder.eval()(img)\n",
    "sampled_ids = decoder.sample(features)[0].cpu().numpy()\n",
    "\n",
    "output_sentence_list = []\n",
    "for index in sampled_ids:\n",
    "    if index == 3: break\n",
    "    if index == 1: pass\n",
    "    else: output_sentence_list.append(vocabulary.index_to_word[index])\n",
    "\n",
    "' '.join(output_sentence_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample result from wellcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/jupyter/datasets/small_images/'\n",
    "\n",
    "wellcome_paths = [base_path + subdir + '/' + image_id\n",
    "                  for subdir in os.listdir(base_path)\n",
    "                  for image_id in os.listdir(base_path + subdir)]\n",
    "\n",
    "wellcome_ids = [path.split('/')[-1].split('.')[0] \n",
    "                for path in wellcome_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_json('/home/jupyter/datasets/works.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.index = metadata['identifiers'].apply(lambda x: x[0]['value']).rename('miro id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.random.choice(wellcome_paths)\n",
    "img = load_image(path, transform=transform).to(device)\n",
    "\n",
    "features = encoder.eval()(img)\n",
    "sampled_ids = decoder.sample(features)\n",
    "sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "output_sentence_list = []\n",
    "\n",
    "for index in sampled_ids:\n",
    "    if index == 3: break\n",
    "    if index == 1: pass\n",
    "    else: output_sentence_list.append(vocabulary.index_to_word[index])\n",
    "\n",
    "print(' '.join(output_sentence_list))\n",
    "\n",
    "Image.fromarray((img.to('cpu').data.numpy() * 255)\n",
    "                .astype(np.uint8)\n",
    "                .reshape(3, 224, 224)\n",
    "                .transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
